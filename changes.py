#=====Fist: play-pause
#=====Pinch: volume (> distance --> lower)
#=====Wrist Height: speed (< height --> lower)

import cv2
import mediapipe as mp
import time
import numpy as np
import threading
import sounddevice as sd    #for music playing
import librosa              #for music speed

#===================================================================

#Audio setup
audio, sr = librosa.load("music.mp3", sr=None, mono=True)
music_speed = 1.0
music_playing = False

#=====================================================================

#Haptics setup
class HapticManager:
    def __init__(self):
        try:
            from bhaptics import haptic_player
            self.player = haptic_player.HapticPlayer()
            self.connected = True
            print("âœ… bHaptics Connected")
        except:
            self.connected = False
            print("âš ï¸ Running in Simulation Mode (No Device Found)")
    
    def play_pinch(self, intensity):
        """Î’Î±ÏƒÎ¹ÎºÎ® Î´ÏŒÎ½Î·ÏƒÎ· Î±Î½Î¬Î»Î¿Î³Î± Î¼Îµ Ï„Î·Î½ Î­Î½Ï„Î±ÏƒÎ· (0-100)"""
        if self.connected:
            pass
#========================================================================

def music_loop():
    global music_speed, music_playing

    pos = 0
    chunk_size = 2048

    while True:
        if not music_playing:
            sd.stop()
            sd.sleep(50)
            continue

        chunk = audio[pos:pos + chunk_size]

        if len(chunk) < 32:
            pos = 0
            continue

        stretched = librosa.effects.time_stretch(chunk.astype(np.float32), music_speed)
        sd.play(stretched, sr, blocking=True)

        pos += int(chunk_size * music_speed)
        sd.sleep(5)

#=================================================================================        

class GestureManager:
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            min_detection_confidence=0.8,
            min_tracking_confidence=0.8,
            max_num_hands=1
        )
        self.mp_draw = mp.solutions.drawing_utils
        self.last_fist_time = 0
        self.fist_cooldown = 1.0

    def get_hand_data(self, frame):
        """Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹ Ï„Î¿ frame ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î± landmarks"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        return results.multi_hand_landmarks if results.multi_hand_landmarks else []

    def recognize_fingers(self, hand_lms):
        """Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î»Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î¿ Ï€Î¿Î¹Î± Î´Î¬Ï‡Ï„Ï…Î»Î± ÎµÎ¯Î½Î±Î¹ Î±Î½Î¿Î¹Ï‡Ï„Î¬"""
        fingers = []
        
        # Î‘Î½Ï„Î¯Ï‡ÎµÎ¹ÏÎ±Ï‚
        if hand_lms.landmark[4].x < hand_lms.landmark[3].x:
            fingers.append(1)
        else:
            fingers.append(0)
            
        # Î¥Ï€ÏŒÎ»Î¿Î¹Ï€Î± Î´Î¬Ï‡Ï„Ï…Î»Î±
        tips = [8, 12, 16, 20]
        pips = [6, 10, 14, 18]
        
        for tip, pip in zip(tips, pips):
            if hand_lms.landmark[tip].y < hand_lms.landmark[pip].y:
                fingers.append(1)
            else:
                fingers.append(0)
        
        return fingers

    def analyze_gesture(self, hand_lms):
        """Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ pinch ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î´ÎµÎ´Î¿Î¼Î­Î½Î±"""
        """pinch: Î±Î½Ï„Î¯Ï‡ÎµÎ¹ÏÎ±Ï‚ + Î´ÎµÎ¯ÎºÏ„Î·Ï‚"""
        thumb_tip = hand_lms.landmark[4]
        index_tip = hand_lms.landmark[8]
        
        # Î‘Ï€ÏŒÏƒÏ„Î±ÏƒÎ· Î³Î¹Î± Pinch
        pinch_dist = ((thumb_tip.x - index_tip.x)**2 + (thumb_tip.y - index_tip.y)**2)**0.5
        pinch_intensity = int(np.interp(pinch_dist, [0.02, 0.15], [100, 0]))
        
        fingers = self.recognize_fingers(hand_lms)
        
        return {
            'fingers': fingers,
            'fingers_count': sum(fingers),
            'pinch_intensity': max(0, min(100, pinch_intensity)),
            'is_fist': sum(fingers) == 0
        }

    def on_fist_detected(self):
        """Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï€Î¿Ï… ÎºÎ±Î»ÎµÎ¯Ï„Î±Î¹ ÏŒÏ„Î±Î½ Î±Î½Î¹Ï‡Î½ÎµÏ…Î¸ÎµÎ¯ Î³ÏÎ¿Î¸Î¹Î¬"""
        """play-pause"""

        global music_playing

        current_time = time.time()
        if current_time - self.last_fist_time > self.fist_cooldown:
            self.last_fist_time = current_time
            music_playing = not music_playing
            print("â–¶ï¸ PLAY" if music_playing else "â¸ PAUSE")
            return True
        return False

#======================================================================================


def main():
    global music_speed

    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    haptics = HapticManager()
    gestures = GestureManager()

    threading.Thread(target=music_loop, daemon=True).start()

    print("\nğŸš€ MAESTRO HAND TRACKER")
    print("ğŸ‘Š Fist = Play / Pause")
    print("ğŸ¤ Pinch = Volume")
    print("âœ‹ Wrist Height = Speed")
    print("Press 'q' to quit\n")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)
        h, w, _ = frame.shape

        hands = gestures.get_hand_data(frame)

        if hands:
            for hand_lms in hands:
                gestures.mp_draw.draw_landmarks(
                    frame, hand_lms, gestures.mp_hands.HAND_CONNECTIONS
                )

                data = gestures.analyze_gesture(hand_lms)

                # ğŸ‘Š Fist â†’ Play / Pause
                if data["is_fist"]:
                    gestures.on_fist_detected()

                # ğŸ¤ Pinch â†’ Volume
                if data["fingers"][0] == 1 and data["fingers"][1] == 1:
                    # Thumb + index pinch â†’ control volume
                    pinch_dist = ((hand_lms.landmark[4].x - hand_lms.landmark[8].x)**2 + (hand_lms.landmark[4].y - hand_lms.landmark[8].y)**2)**0.5
                    volume_target = np.interp(pinch_dist, [0.02, 0.15], [0, 100])
                    current_volume = current_volume * 0.85 + volume_target * 0.15
                    current_volume = max(0, min(100, current_volume))
                    haptics.play_pinch(int(current_volume))
                    print(f"ğŸ”Š Volume: {int(current_volume)}%")

                # âœ‹ Wrist height â†’ Speed
                wrist_y = hand_lms.landmark[0].y  # y of wrist
                target_speed = np.interp(wrist_y, [0.8, 0.2], [0.6, 1.6])
                music_speed = music_speed * 0.9 + target_speed * 0.1
                music_speed = max(0.5, min(2.0, music_speed))
                print(f"ğŸµ Speed: {music_speed:.2f}x")

                # UI
                cv2.putText(frame, f"Speed: {music_speed:.2f}x", (50, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 200, 0), 3)

        cv2.imshow("Maestro Gesture Recognition", frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
